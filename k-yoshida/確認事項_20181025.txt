正則化
Lassoの場合、影響力の弱い説明変数については、係数をゼロにしてしまい
回帰式から消してしまう。
Ridgeの場合、Lassoのように消すことが目的ではなく、二乗値がより大きな値の係数を
小さくする。
この２つの方法により、過学習を防ぐことにつなげる。


神崎さんが取り上げたメルカリの価格予測については、テキストで表現されたカテゴリカルデータについて、あるカテゴリが価格に対してたとえ影響力が弱くても、それを出来るだけ残すように扱い、逆に、極端に影響力が強いカテゴリを抑える方向で正則化(Ridge)をかける方が、全ての商品について、偏りなく学習が進む(過学習を防ぐ)と判断したためと考えられる。
カテゴリカルデータ→ダミー変数にして重回帰を行う場合に、Ridgeを使うことは、必然的と言えるのか、あるいは、メルカリの価格予測について効果があったドメインスペシフィックなノウハウと言えるのか？
http://www.st.nanzan-u.ac.jp/info/gr-thesis/2012/09se067.pdf
上記資料からは、決して必然的ではなく、ドメインスペシフィックなものと考えられる。

LightGBM
Microsoft が開発した勾配ブースティング (Gradient Boosting) アルゴリズムを扱うためのフレームワーク。回帰にも分類にも応用できる。
勾配ブースティング
(以下、http://smrmkt.hatenablog.jp/entry/2015/04/28/210039からの引用)
複数の弱学習器を組み合わせるアンサンブル学習には，いくつかの手法がありますが，ブースティングは逐次的に弱学習器を構築していく手法です．逐次的というのは，弱学習器を1つずつ順番に構築していくという意味です．新しい弱学習器を構築する際に，それまでに構築されたすべての弱学習器の結果を利用します．そのためすべての弱学習器が独立に学習されるバギングと比べると，計算を並列化できず学習に時間がかかります．

ブースティングでは，各ステップごとに弱学習器を構築して損失関数を最小化します．その際に，各学習データの扱いはずっと平等ではありません．各学習データのうち，前のステップで間違って識別されたものへのウェイトを重くして，次のステップで間違ったものをうまく識別できるようにしていきます．

各ステップ内でやることは，ようするに損失関数の最小化問題です．これだけ切り出せば，通常の最適化問題とそれほど大きくは変わりません．最適化問題でよく使われる最急降下法やニュートン法なんかをまとめて，勾配降下法ということができます．勾配ブースティングでやっていることは，各ステップのパラメタ最適化の際に，勾配降下法を用いているというだけのことです．もちろん数学的にはいろいろあるわけですけれども，大枠としてはそれだけです．勾配を求めて学習していく，という形をとるので，損失関数をパラメタ行列で微分してあげるのを繰り返して，所定回数に達したらおしまいです．

勾配ブースティングでよく使われるのは，弱識別器に決定木をもちいたGBDT(Gradient Boosting Decision Tree)です．
(引用終わり)

リッジ回帰とLightGBMの関係について
RidgeのKernelsは、MercariやAvitoといったWebサイト関係のCompetitionを題材としたものが多く、その場合にFeatureImportanceを分析するための定石となるアルゴリズム選択の候補と考えられる

参考：機械学習の評価関数
https://data.gunosy.io/entry/2016/08/05/115345


決定木における欠損値の扱い
参考：
http://heartland.geocities.jp/ecodata222/ed/edj1-5-6-2.html
http://heartland.geocities.jp/ecodata222/ed/edj1-5-3-5.html


参考：PandasがMissing Dataをどのように扱うか
Python Data Science Handbook p.119～


決定木における枝刈り
基本的な考え方：http://www.randpy.tokyo/entry/decision_tree_theory_pruning
正則化パラメータαの値に基づいて決定木が作られる


どのアルゴリズムを選べばいいか？
参考：Scikit-Learnのチュートリアル
http://scikit-learn.org/stable/tutorial/machine_learning_map/
参考：仕事ではじめる機械学習　p.9
「クラスタリングなどの教師なし学習や散布図行列などを使って事前に可視化してあたりをつけ、どういった方法で解けるのかを考えます。」


クラスタリングについての参考文献
http://www.kamishima.net/archive/clustering.pdf

