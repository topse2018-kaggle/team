正規分布(確率密度関数)とシグモイド関数の関係について
シグモイド関数ではなく、正規分布の累積分布関数を用いたモデルのことをプロビット回帰と呼ぶ。
しかし、プロビット回帰を用いてニューロンをモデル化することはほぼない。なぜならば、正規分布の累積分布関数から確率的勾配降下法を適用するのに必要な勾配計算を行うのは煩雑かつ困難なため。
そこで、正規分布の累積分布関数に形が似ており、計算が簡単に行える関数として、シグモイド関数が用いられるようになった。
参照：詳解ディープラーニング　p.105


勾配降下法でx(k)からx(k+1)を算出する式について
x(k)における微分値(傾き)が大きければ、傾き=0になるのは、まだ当分先のことと捉え、x(k)から離れた場所にx(k+1)を取る。
x(k)における微分値(傾き)が小さければ、その逆になる。


損失関数の勾配と活性化関数の関係について
シグモイド関数の場合には、損失関数の勾配(Wによる偏微分)を求める際に便利な式
f'(x)=f(x)(1-f(x))
が成立することがわかっているが(参照：詳解ディープラーニング　p.91)、勾配消失を解消するための双曲線正接関数やReLUでは、同様の式が成り立つのか？
少なくとも、ReLUでは成立しない。

勾配消失問題を解消するために
シグモイド関数に代わり次のような累積分布関数を用いる
双曲線正接関数
ReLU

(バッチ)勾配降下法
↓
ミニバッチ勾配降下法
↑
確率的勾配降下法


SVMのハイパーパラメータについて
C：コストパラメータ
γ：RBFカーネルのパラメータ
"C が小さいときは決定領域の中に多くの誤分類点を含んでいる一方で, Cが大きいときは決定領域内の誤分類点が少なくなっています."
"γが小さいときの決定境界は単純な決定境界(直線)である一方で, γが大きいときの決定境界は複雑な形をしています."
引用元：
https://qiita.com/sz_dr/items/f3d6630137b184156a67


ベイズ最適化について
ハイパーパラメータ探索のための手法
参照：
https://book.mynavi.jp/manatee/detail/id=59393
irisデータセットを使って、SVMのCとγをグリッドサーチよりも効率よく探索する。

Pythonでベイズ最適化を行うには
参照：
https://blog.amedama.jp/entry/2018/08/18/233841


SVMなどの2クラス分類木で多クラス問題を解く方法について
・一対他分類器(one-versus-the-rest classifier)
i=1,…,k-1 の各クラス i それぞれについて，クラス i なら 1を，その他のクラスなら 0 を識別する2値分類器を学習する． 
クラス k については，k-1個の分類器が全て 0 を出力すれば，クラスkと分かる．
複数の分類器が 1 を出力したとき，最終的な解をどれにするか決定出来ない場合もある．
・一対一分類器(one-versus-one classifier)
k(k-1)/2 個のクラスの対 i,j∈1,…,j，i≠j について，クラス i と j とを識別する分類器を作る． 
最終的なクラスは多数決によって決める．
クラス A，B，C があるときに，AとBではA，BとCではB，CとAではCと分類されて，循環が生じ最終結果が決定できない場合がある．
引用元：
http://ibisforest.org/index.php?%E5%A4%9A%E3%82%AF%E3%83%A9%E3%82%B9


PythonコーディングTips
DataFrameを対象としたiteration系のメソッド(ex. iterrows())は、データ量が多い場合には、性能劣化の原因になるので、利用しないこと。
その代わりに、DataFrame.valuesメソッドを使って、Numpy.arrayの2次元配列にして、ループで処理を回した方が、性能は良くなる。
